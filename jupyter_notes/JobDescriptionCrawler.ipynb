{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c892d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiofiles in /Users/yidan/opt/anaconda3/envs/deeplearn/lib/python3.9/site-packages (23.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013baed3-17e2-4a26-8fe9-d3dbad607ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dz/_kg4kpk160d653__f0w9lxbm0000gn/T/ipykernel_40024/1702376316.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import scrapy\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import settings\n",
    "\n",
    "class LinkedInJobDescriptionSpider(scrapy.Spider):\n",
    "    name = \"linkedin_job\"\n",
    "    \n",
    "    keywords = settings.KEYWORDS #'software_engineer'\n",
    "\n",
    "    # job_pages = [\"https://www.linkedin.com/jobs/view/python-developer-internship-at-mindpal-3703089625?refId=w8fjclBo8vHOKvOm6qTGIA%3D%3D&trackingId=11l%2BLLmpE5BEVgxUIhaY2A%3D%3D&position=19&pageNum=0&trk=public_jobs_jserp-result_search-card\"]\n",
    "\n",
    "    def start_requests(self):\n",
    "        job_index_tracker = 0\n",
    "        first_keyword = 0\n",
    "\n",
    "        keyword = self.keywords[first_keyword].unquote().replace('%2B', '_')\n",
    "\n",
    "        self.readUrlsFromJobsFile(keyword)\n",
    "        first_url = self.job_pages[job_index_tracker]\n",
    "\n",
    "        yield scrapy.Request(url=first_url, callback=self.parse, meta={'job_index_tracker': job_index_tracker, 'first_keyword': first_keyword})\n",
    "        \n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        job_index_tracker = response.meta['job_index_tracker']\n",
    "        first_keyword = response.meta['first_keyword']\n",
    "        keyword = self.keywords[first_keyword].unquote().replace('%2B', '_')\n",
    "\n",
    "        print('***************')\n",
    "        print('****** Scraping page ' + str(job_index_tracker+1) + ' of ' + str(len(self.job_pages)) + ' for keyword ' + keyword + ' ******')\n",
    "        print('***************')\n",
    "\n",
    "        job_item = {}\n",
    "\n",
    "        url_parts = response.url.split('/')\n",
    "        jobs_index = url_parts.index('view')\n",
    "        job_code = url_parts[jobs_index + 1].split(\"?refId\")[0]\n",
    "        job_id = job_code.split('-')[-1]\n",
    "\n",
    "        # Extract job basic details\n",
    "        job_item['job_id'] = job_id\n",
    "\n",
    "        # print(response.text)\n",
    "        try:\n",
    "            job_item['job_title'] = response.css(\"h1::text\").get(default='not-found').strip()\n",
    "            job_item['job_link'] = response.url\n",
    "            job_item['company_name'] = response.css('.topcard__org-name-link::text').get(default='not-found').strip()\n",
    "            job_item['company_link'] = response.css('.topcard__org-name-link::attr(href)').get(default='not-found').strip()\n",
    "            job_item['job_location'] = response.css('.topcard__flavor--bullet::text').get(default='not-found').strip()\n",
    "\n",
    "            script_content = response.xpath('//script[@type=\"application/ld+json\"]/text()').get()\n",
    "            soup = BeautifulSoup(script_content, 'html.parser')\n",
    "            description_html = soup.get_text()\n",
    "            description_data = json.loads(description_html)\n",
    "            job_item['date_posted'] = description_data['datePosted']\n",
    "            job_item['job_description'] = description_data['description']\n",
    "        \n",
    "        except TypeError:\n",
    "            self.logger.warning(\"Script content is None. Skipping parsing.\")\n",
    "        except IndexError:\n",
    "            self.logger.warning(\"Index out of range. Skipping parsing.\")\n",
    "\n",
    "        job_item['search_keywords'] = keyword\n",
    "        job_item['job_code'] = job_code\n",
    "\n",
    "        self.save_to_csv(job_item)\n",
    "\n",
    "        yield job_item\n",
    "\n",
    "        job_index_tracker = job_index_tracker + 1\n",
    "\n",
    "        if job_index_tracker <= (len(self.job_pages)-1):\n",
    "            next_url = self.job_pages[job_index_tracker]\n",
    "            yield scrapy.Request(url=next_url, callback=self.parse, meta={'job_index_tracker': job_index_tracker, 'first_keyword': first_keyword})\n",
    "        else:\n",
    "            first_keyword = first_keyword + 1\n",
    "            if first_keyword < len(self.keywords):\n",
    "                job_index_tracker = 0\n",
    "                self.readUrlsFromJobsFile(self.keywords[first_keyword].unquote().replace('%2B', '_'))\n",
    "                next_url = self.job_pages[job_index_tracker]\n",
    "                yield scrapy.Request(url=next_url, callback=self.parse, meta={'job_index_tracker': job_index_tracker, 'first_keyword': first_keyword})\n",
    "            else:\n",
    "                print('***************')\n",
    "                print('****** Finished scraping all jobs ******')\n",
    "                print('***************')\n",
    "\n",
    "\n",
    "    def save_to_csv(self, item):\n",
    "        # Define the output directory where CSV files will be saved\n",
    "        output_dir = 'data/{}'.format(item['search_keywords'])\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Define the CSV filename based on a unique identifier in the item\n",
    "        filename = os.path.join(output_dir, f'{item[\"job_code\"]}.csv')  # Assuming 'title' exists in the item\n",
    "        df = pd.DataFrame([item])\n",
    "\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    def readUrlsFromJobsFile(self, keyword):\n",
    "        filename = os.path.join('data_url', '{}.csv'.format(keyword)) \n",
    "        self.job_pages = []\n",
    "        with open(filename, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for job in reader:\n",
    "                if job['url'] != 'not-found':\n",
    "                    self.job_pages.append(job['url'])\n",
    "            \n",
    "        #remove any duplicate links - to prevent spider from shutting down on duplicate\n",
    "        self.job_pages = list(set(self.job_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68be972f-b433-4ebc-9350-206255b2b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 15:47:16 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: scrapybot)\n",
      "2024-02-15 15:47:16 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.9.18 (main, Sep 11 2023, 08:38:23) - [Clang 14.0.6 ], pyOpenSSL 23.2.0 (OpenSSL 3.0.12 24 Oct 2023), cryptography 41.0.7, Platform macOS-10.16-x86_64-i386-64bit\n",
      "2024-02-15 15:47:16 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-02-15 15:47:16 [py.warnings] WARNING: /Users/yidan/opt/anaconda3/envs/deeplearn/lib/python3.9/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-02-15 15:47:16 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-02-15 15:47:16 [scrapy.extensions.telnet] INFO: Telnet Password: fe9c597d67addc94\n",
      "2024-02-15 15:47:16 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-02-15 15:47:16 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-02-15 15:47:16 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-02-15 15:47:16 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-02-15 15:47:16 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-02-15 15:47:16 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-02-15 15:47:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-02-15 15:47:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6034\n",
      "2024-02-15 15:47:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.linkedin.com/jobs/view/data-analyst-full-time-remote-beginner-level-at-opinion-focus-panel-llc-3829946758?refId=domNMOSzr5HAHF2CN69BMw%3D%3D&trackingId=n966HGeAs%2FUeZ6RUdU42sg%3D%3D&position=7&pageNum=2&trk=public_jobs_jserp-result_search-card> (referer: None)\n",
      "2024-02-15 15:47:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.linkedin.com/jobs/view/data-analyst-full-time-remote-beginner-level-at-opinion-focus-panel-llc-3829946758?refId=domNMOSzr5HAHF2CN69BMw%3D%3D&trackingId=n966HGeAs%2FUeZ6RUdU42sg%3D%3D&position=7&pageNum=2&trk=public_jobs_jserp-result_search-card>: HTTP status code is not handled or not allowed\n",
      "2024-02-15 15:47:17 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-02-15 15:47:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 452,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 55775,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 0.816844,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 2, 15, 21, 47, 17, 580584, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 319687,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/404': 1,\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 155787264,\n",
      " 'memusage/startup': 155783168,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2024, 2, 15, 21, 47, 16, 763740, tzinfo=datetime.timezone.utc)}\n",
      "2024-02-15 15:47:17 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "process.crawl(LinkedInJobDescriptionSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc22ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c11680-e4e9-4ca4-9754-af8d6a214da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
