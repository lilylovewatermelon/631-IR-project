{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdebad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install beautifulsoup4\n",
    "# %pip install scrapy\n",
    "# %pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6b6514-dc8c-42e9-a52b-b2f34dc02ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dz/_kg4kpk160d653__f0w9lxbm0000gn/T/ipykernel_27016/2108451895.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from urllib.parse import unquote\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "class LinkedJobsSpider(scrapy.Spider):\n",
    "    name = \"linkedin_jobs\"\n",
    "    api_url_template = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={}&location=United%2BStates&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&start={}'\n",
    "    jobs_url = []\n",
    "    urls = []\n",
    "\n",
    "    # custom_settings = {\n",
    "\t# \t'FEEDS': { 'url.csv': { 'format': 'csv',}}\n",
    "\t# \t}\n",
    "\n",
    "    def start_requests(self):\n",
    "\n",
    "        # keywords = ['software%20engineer', 'data%20scientist', 'data%20engineer', 'machine%20learning%20engineer', 'artificial%20intelligence%20engineer', 'python']\n",
    "        keywords = 'python'#'software%2Bengineer'\n",
    "\n",
    "        output_dir = 'data_url' \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        search_keywords = unquote(keywords).replace('%2B', '_')\n",
    "        filename = os.path.join(output_dir, '{}.csv'.format(search_keywords))  # Assuming 'title' exists in the item\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(['url', 'job_code', 'job_id'])\n",
    "\n",
    "        for page in range(3):\n",
    "            start_value = page * 25\n",
    "            url = self.api_url_template.format(keywords, start_value)\n",
    "            # search_keywords = unquote(keywords).replace('%2B', '_')\n",
    "            yield scrapy.Request(url=url, callback=self.parse_job, meta={'start_value': start_value, 'search_keywords': search_keywords})\n",
    "\n",
    "\n",
    "    def parse_job(self, response):\n",
    "        start_value = response.meta.get('start_value')\n",
    "        search_keywords = response.meta.get('search_keywords')\n",
    "  \n",
    "\n",
    "        jobs = response.css(\"li\")\n",
    "\n",
    "        num_jobs_returned = len(jobs)\n",
    "        # count += num_jobs_returned\n",
    "        print(f\"******* Num Jobs Returned for start={start_value} *******\")\n",
    "        print(num_jobs_returned)\n",
    "        # print(\"total: {}\".format(self.count))\n",
    "        print('*****')\n",
    "\n",
    "        output_dir = 'data_url' \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        filename = os.path.join(output_dir, '{}.csv'.format(search_keywords))\n",
    "        # filename = response.meta.get('filename')\n",
    "        urls = []\n",
    "\n",
    "        for job in jobs:\n",
    "            # url_item = {}\n",
    "\n",
    "            detail_url = job.css(\".base-card__full-link::attr(href)\").get(default='not-found').strip()\n",
    "\n",
    "            url_parts = detail_url.split('/')\n",
    "            jobs_index = url_parts.index('view')\n",
    "            \n",
    "            job_code = url_parts[jobs_index + 1].split(\"?refId\")[0]\n",
    "            job_id = job_code.split('-')[-1]\n",
    "\n",
    "            url_item = {\n",
    "                'url': detail_url,\n",
    "                'job_code': job_code,\n",
    "                'job_id': job_id\n",
    "            }\n",
    "\n",
    "            urls.append(url_item)\n",
    "\n",
    "        df = pd.DataFrame(urls)\n",
    "        with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            df.to_csv(csvfile, header=not csvfile.tell(), index=False)\n",
    "            # csv_writer = csv.writer(csvfile)\n",
    "            \n",
    "            # csv_writer.writerows(df)\n",
    "                \n",
    "        # print(urls)            \n",
    "        # search_keywords = unquote(keywords).replace('%2B', '_')\n",
    "        # filename = os.path.join(output_dir, '{}.csv'.format(search_keywords))  # Assuming 'title' exists in the item\n",
    "        # with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        #     csv_writer = csv.writer(csvfile)\n",
    "        #     csv_writer.writerow(['url', 'job_code', 'job_id'])\n",
    "\n",
    "            \n",
    "                \n",
    "                # Check if file is empty, if yes, write header row\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f89ece-5851-4b59-aa08-b572813b515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 23:18:57 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: scrapybot)\n",
      "2024-02-14 23:18:57 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.9.18 (main, Sep 11 2023, 08:38:23) - [Clang 14.0.6 ], pyOpenSSL 23.2.0 (OpenSSL 3.0.12 24 Oct 2023), cryptography 41.0.7, Platform macOS-10.16-x86_64-i386-64bit\n",
      "2024-02-14 23:18:57 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-02-14 23:18:57 [py.warnings] WARNING: /Users/yidan/opt/anaconda3/envs/deeplearn/lib/python3.9/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-02-14 23:18:57 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-02-14 23:18:57 [scrapy.extensions.telnet] INFO: Telnet Password: 256eedbd5e15b78b\n",
      "2024-02-14 23:18:57 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-02-14 23:18:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-02-14 23:18:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-02-14 23:18:57 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-02-14 23:18:57 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-02-14 23:18:57 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-02-14 23:18:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-02-14 23:18:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6031\n",
      "2024-02-14 23:18:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=python&location=United%2BStates&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&start=50> (referer: None)\n",
      "2024-02-14 23:18:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=python&location=United%2BStates&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&start=0> (referer: None)\n",
      "2024-02-14 23:18:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=python&location=United%2BStates&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&start=25> (referer: None)\n",
      "2024-02-14 23:18:58 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-02-14 23:18:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1133,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 26621,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 3,\n",
      " 'elapsed_time_seconds': 1.308916,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 2, 15, 5, 18, 58, 708767, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 221171,\n",
      " 'httpcompression/response_count': 3,\n",
      " 'log_count/DEBUG': 4,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 144445440,\n",
      " 'memusage/startup': 144445440,\n",
      " 'response_received_count': 3,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2024, 2, 15, 5, 18, 57, 399851, tzinfo=datetime.timezone.utc)}\n",
      "2024-02-14 23:18:58 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Num Jobs Returned for start=50 *******\n",
      "24\n",
      "*****\n",
      "******* Num Jobs Returned for start=0 *******\n",
      "24\n",
      "*****\n",
      "******* Num Jobs Returned for start=25 *******\n",
      "25\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "settings = get_project_settings()\n",
    "process = CrawlerProcess(settings)\n",
    "process.crawl(LinkedJobsSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
